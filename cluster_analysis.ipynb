{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stel dat we een dataset hebben met wijkenkenmerken, bijvoorbeeld demografische en sociaaleconomische data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('wijken.csv')  # Bijvoorbeeld een dataset met CBS wijkenkenmerken\n",
    "display(data.head())\n",
    "#check datatypes\n",
    "display(data.info())\n",
    "#check missende waardes\n",
    "print(data.isna().sum().sum())\n",
    "object_columns_df = data.select_dtypes(include=['object'])\n",
    "print(object_columns_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('Codering_3')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_elbow_curve(X, max_k=10):\n",
    "    \"\"\"Generate an elbow plot to find the optimal number of clusters for KMeans.\"\"\"\n",
    "    # List to hold the inertia (sum of squared distances) for each value of k\n",
    "    inertia = []\n",
    "\n",
    "    # Fit KMeans for different values of k (from 1 to max_k)\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        inertia.append(kmeans.inertia_)  # Inertia is the sum of squared distances to the closest cluster center\n",
    "\n",
    "    # Plot the inertia values for each k\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_k + 1), inertia, 'bo-', markersize=8)\n",
    "    plt.title('Elbow Plot for Optimal k')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    plt.xticks(range(1, max_k + 1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_elbow_curve(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standaard pipeline voor clusteranalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_standard_scaling(data):\n",
    "    \"\"\"Standardize the dataset (mean=0, variance=1).\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def apply_kmeans(data, n_clusters=8):\n",
    "    \"\"\"Apply KMeans clustering and return labels and centroids.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.labels_, kmeans.cluster_centers_\n",
    "\n",
    "def apply_pca(data, n_components=2):\n",
    "    \"\"\"Reduce the dimensionality of the dataset using PCA.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "def plot_clusters(pca_data, labels, centroids):\n",
    "    \"\"\"Plot the PCA-reduced data with cluster assignments.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the data points with their cluster labels\n",
    "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    \n",
    "    # Plot the cluster centroids\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', label='Centroids', marker='X')\n",
    "    \n",
    "    plt.title('PCA of Clusters')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "def plot_centroid_analysis(centroids):\n",
    "    # Plot the centroid differences for each feature\n",
    "    pass\n",
    "\n",
    "# Main pipeline\n",
    "\n",
    "def main(data):\n",
    "    # Step 1: Standardize the data (optional, but recommended)\n",
    "    data = apply_standard_scaling(data)\n",
    "\n",
    "    # Step 2: Apply KMeans clustering\n",
    "    k = 3  # Number of clusters\n",
    "    labels, centroids = apply_kmeans(data, n_clusters=k)\n",
    "\n",
    "    # Step 3: Apply PCA for 2D visualization\n",
    "    pca_data = apply_pca(data)\n",
    "\n",
    "    # Step 4: Plot the clusters\n",
    "    # Since centroids are in the original space, we need to apply PCA to centroids as well\n",
    "    pca_centroids = apply_pca(centroids, n_components=2)\n",
    "    plot_clusters(pca_data, labels, pca_centroids)\n",
    "    plot_centroid_analysis(centroids)\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = main(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoe ziet het er uit met tsne (niet linear, kan geen centroids gebruiken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_standard_scaling(data):\n",
    "    \"\"\"Standardize the dataset (mean=0, variance=1).\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def apply_kmeans(data, n_clusters=8):\n",
    "    \"\"\"Apply KMeans clustering and return labels.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def apply_tsne(data, n_components=2, perplexity=30):\n",
    "    \"\"\"Reduce the dimensionality of the dataset using t-SNE.\"\"\"\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)\n",
    "    return tsne.fit_transform(data)\n",
    "\n",
    "def plot_clusters(tsne_data, labels):\n",
    "    \"\"\"Plot the t-SNE-reduced data with cluster assignments.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the data points with their cluster labels\n",
    "    scatter = plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    plt.title('t-SNE of Clusters')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "# Main pipeline\n",
    "\n",
    "def main(data):\n",
    "    # Step 1: Standardize the data\n",
    "    data = apply_standard_scaling(data)\n",
    "\n",
    "    # Step 2: Apply KMeans clustering\n",
    "    k = 3  # Number of clusters\n",
    "    labels = apply_kmeans(data, n_clusters=k)\n",
    "\n",
    "    # Step 3: Apply t-SNE for 2D visualization\n",
    "    tsne_data = apply_tsne(data, perplexity=10)  # Adjust perplexity based on your dataset size\n",
    "\n",
    "    # Step 4: Plot the clusters\n",
    "    plot_clusters(tsne_data, labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = main(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train een random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "def feature_importance(X, clusters):\n",
    "    clf.fit(X, clusters)\n",
    "\n",
    "    # Feature importance opvragen\n",
    "    importances = clf.feature_importances_\n",
    "\n",
    "    # Visualiseer de feature importance\n",
    "    feature_names = X.columns  # Stel dat je dataset kolomnamen heeft\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "    # Sorteer op basis van importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot de feature importance\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 20))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance van Cluster Classificatie')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "feature_importance(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de code aan te passen om DBSCAN (Density-Based Spatial Clustering of Applications with Noise) te gebruiken in plaats van KMeans, moet je de volgende aanpassingen maken:\n",
    "\n",
    "- **DBSCAN gebruikt geen centroiden**, in tegenstelling tot KMeans. We hoeven dus geen centroiden te berekenen of te visualiseren.\n",
    "- **DBSCAN clustering**: DBSCAN wijst een label van `-1` toe aan ruispunten die tot geen enkele cluster behoren. We moeten hiermee rekening houden bij het plotten.\n",
    "- **PCA voor dimensiereductie**: We zullen nog steeds PCA gebruiken om de clusters in 2D te visualiseren.\n",
    "- **DBSCAN parameters**: DBSCAN heeft twee belangrijke parameters: `eps` (de maximale afstand tussen twee samples om als buren te worden beschouwd) en `min_samples` (het minimum aantal punten dat nodig is om een dichte regio te vormen). Je moet mogelijk deze parameters aanpassen op basis van je dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_standard_scaling(data):\n",
    "    \"\"\"Standardize the dataset (mean=0, variance=1).\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def apply_dbscan(data, eps=0.5, min_samples=5):\n",
    "    \"\"\"Apply DBSCAN clustering and return labels.\"\"\"\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "def apply_pca(data, n_components=2):\n",
    "    \"\"\"Reduce the dimensionality of the dataset using PCA.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "def plot_clusters(pca_data, labels):\n",
    "    \"\"\"Plot the PCA-reduced data with cluster assignments.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the data points with their cluster labels\n",
    "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    plt.title('PCA of DBSCAN Clusters')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "# Main pipeline\n",
    "\n",
    "def main(data):\n",
    "    # Step 1: Standardize the data\n",
    "    data = apply_standard_scaling(data)\n",
    "\n",
    "    # Step 2: Apply DBSCAN clustering\n",
    "    labels = apply_dbscan(data, eps=0.05, min_samples=5)  # Adjust eps and min_samples based on your dataset\n",
    "\n",
    "    # Step 3: Apply PCA for 2D visualization\n",
    "    pca_data = apply_pca(data)\n",
    "\n",
    "    # Step 4: Plot the clusters\n",
    "    plot_clusters(pca_data, labels)\n",
    "    return labels\n",
    "\n",
    "# Assuming your DataFrame 'data' is available and cleaned\n",
    "labels = main(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu pas ik de code aan voor HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_standard_scaling(data):\n",
    "    \"\"\"Standardize the dataset (mean=0, variance=1).\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def apply_hac(data, n_clusters=3, linkage='ward'):\n",
    "    \"\"\"Apply Hierarchical Agglomerative Clustering and return labels.\"\"\"\n",
    "    hac = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "    labels = hac.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "def apply_pca(data, n_components=2):\n",
    "    \"\"\"Reduce the dimensionality of the dataset using PCA.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(data)\n",
    "\n",
    "def plot_clusters(pca_data, labels):\n",
    "    \"\"\"Plot the PCA-reduced data with cluster assignments.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the data points with their cluster labels\n",
    "    scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    plt.title('PCA of HAC Clusters')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "# Main pipeline\n",
    "\n",
    "def main(data):\n",
    "    # Step 1: Standardize the data\n",
    "    data = apply_standard_scaling(data)\n",
    "\n",
    "    # Step 2: Apply Hierarchical Agglomerative Clustering (HAC)\n",
    "    labels = apply_hac(data, n_clusters=2, linkage='ward')  # Adjust n_clusters and linkage as needed\n",
    "\n",
    "    # Step 3: Apply PCA for 2D visualization\n",
    "    pca_data = apply_pca(data)\n",
    "\n",
    "    # Step 4: Plot the clusters\n",
    "    plot_clusters(pca_data, labels)\n",
    "    return labels\n",
    "\n",
    "# Assuming your DataFrame 'data' is available and cleaned\n",
    "labels = main(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dendrogram(data, method='ward'):\n",
    "    \"\"\"Plot dendrogram for hierarchical clustering.\"\"\"\n",
    "    Z = linkage(data, method=method)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(Z)\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "# To use this, simply call it in the main pipeline before or after HAC clustering:\n",
    "plot_dendrogram(data, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITANN",
   "language": "python",
   "name": "itann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
