{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4e9474",
   "metadata": {},
   "source": [
    "# Demo notebook for a pipeline functions\n",
    "\n",
    "This demonstration notebook demonstrates the usage of a pipeline function. See also Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html). The sklearn pipeline function is a tool in machine learning that simplifies the workflow by encapsulating all the steps involved in a single object. It offers advantages such as simplicity, reproducibility, efficiency, flexibility, and integration. \n",
    "\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object (the method to be executed). \n",
    "\n",
    "This notebook demonstrates several use cases, comparing the single steps to the pipeline step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e0da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e00da7",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "For this notebook we use lifelines data. Lifelines is a large, multi-generational, prospective cohort study that includes over 167,000 participants (10%) from the northern population of the Netherlands. Within this cohort study the participants are followed over a 30-year period. Every five years, participants visit one of the Lifelines sites in the northern parts of the Netherlands for an assessment. During these visits, several physical measurements are taken and different biomaterials are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab95e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/lifelines.txt', sep = '\\t').set_index('ZIPCODE')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3776e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 71)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30bc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy to compare manual way with pipeline way\n",
    "data_manual = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abb90a",
   "metadata": {},
   "source": [
    "## Use case preprocessing\n",
    "\n",
    "First we will demonstrate the manual steps, the log tranformation and the scaling. Then we demonstrate the pipeline way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afee58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_2log(df):\n",
    "    skew_columns = (df.skew().sort_values(ascending=False))\n",
    "    skew_columns = skew_columns.loc[skew_columns > 0.75]\n",
    "    #print(skew_columns)\n",
    "    # Perform log transform on skewed columns\n",
    "    for col in skew_columns.index.tolist():\n",
    "        df[col] = np.log1p(df[col])\n",
    "    #print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305328c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004747867584228516\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# single step log transformation\n",
    "data_manual = transform_2log(data_manual)\n",
    "# single step scaling\n",
    "mms = MinMaxScaler()\n",
    "data_manual = mms.fit_transform(data_manual)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e1f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003876209259033203\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# The pipeline\n",
    "prep = Pipeline([('log1p', FunctionTransformer(transform_2log)), \n",
    "                 ('minmaxscale', MinMaxScaler())]\n",
    "               )\n",
    "\n",
    "# Convert the original data\n",
    "data_pipe = prep.fit_transform(df)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93a375",
   "metadata": {},
   "source": [
    "Note that the `pipeline` uses the `fit_transform()` function. The MinMaxScaler does have the fit_transform function but the np.log1p function does not have this. We therefore use the sklearn `FunctionTransformer` function. The `FunctionTransformer` is a class in the sklearn.preprocessing module that allows you to apply an arbitrary function to your data. It is commonly used as a preprocessing step in sklearn pipelines.\n",
    "\n",
    "The FunctionTransformer constructor takes as input a callable, which can be any Python function or lambda expression. When applied to a dataset, the FunctionTransformer simply calls the specified function on the input data and returns the transformed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9811d",
   "metadata": {},
   "source": [
    "\n",
    "`np.allclose` is a function in the NumPy library that compares two arrays element-wise and returns True if all corresponding pairs of elements are within a specified tolerance of each other, and False otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2db22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if values are the same, pipeline works same as the manual steps\n",
    "np.allclose(data_pipe, data_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e41a82",
   "metadata": {},
   "source": [
    "## Extend pipeline with PCA\n",
    "We can use the preprocessing pipeline in a new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/lifelines.txt', sep = '\\t').set_index('ZIPCODE')\n",
    "data_manual = df.copy()\n",
    "# single step log transformation\n",
    "data_manual = transform_2log(data_manual)\n",
    "# single step scaling\n",
    "mms = MinMaxScaler()\n",
    "data_scaled = mms.fit_transform(data_manual)\n",
    "#single step PCA\n",
    "pca = PCA(2, random_state=42)\n",
    "result_manual_pca = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cf5c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use case PCA\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', prep),\n",
    "    ('pca', PCA(2, random_state=42))\n",
    "])\n",
    "\n",
    "results_pipe_pca = pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2034e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(result_manual_pca, results_pipe_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97cbf0",
   "metadata": {},
   "source": [
    "# Use a pipeline for classification\n",
    "\n",
    "With pipelines We can follow a structured approach to model development, starting with data preprocessing and feature scaling. To ensure our model generalizes well to unseen data, we will split the dataset into training and testing sets. We will then build a machine learning pipeline that includes a StandardScaler for feature scaling and a RandomForestClassifier for classification.\n",
    "\n",
    "`Pipeline` is often used in combination with `GridSearchCV` for hyperparameter tuning because it allows us to encapsulate all the preprocessing and modeling steps into a single object. This can be useful for several reasons:\n",
    "\n",
    "1. Convenience: Instead of manually performing each preprocessing step and modeling step separately, we can combine them into a single object and pass them as an argument to `GridSearchCV`. This can make our code simpler and easier to read.\n",
    "\n",
    "2. Reproducibility: By encapsulating all the preprocessing and modeling steps into a single object, we can ensure that the same steps are applied consistently to the data, regardless of how many times we run the code.\n",
    "\n",
    "3. Avoiding data leakage: When performing hyperparameter tuning, it's important to ensure that the evaluation of each parameter combination is done using only the training data, not the validation data. By including the preprocessing steps in the `Pipeline` object, we can ensure that the same preprocessing steps are applied to both the training and validation data, preventing data leakage.\n",
    "\n",
    "`GridSearchCV` is used to search over a grid of hyperparameters to find the best combination of hyperparameters that maximizes some performance metric, see `sklearn.metrics`. By using `Pipeline` in combination with `GridSearchCV`, we can search over a grid of hyperparameters for both the preprocessing and modeling steps simultaneously. This can be a powerful technique for optimizing the performance of machine learning models.\n",
    "\n",
    "We can configure `GridSearchCV` with: \n",
    "- estimator: estimator object being used, can be the result of a pipeline\n",
    "- param_grid: dictionary that contains all of the parameters to try\n",
    "- scoring: evaluation metric to use when ranking results\n",
    "- cv: cross-validation, the number of cv folds for each combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6303ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best Parameters from GridSearchCV:\n",
      "{'classifier__max_depth': 4, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# load the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#split train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building a Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 150],\n",
    "    'classifier__max_depth': [2, 4, 6],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "# Best parameters\n",
    "print(\"\\nBest Parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predictions\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
